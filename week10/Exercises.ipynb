{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eb0ce6d",
   "metadata": {},
   "source": [
    "# Week 10 Exercises\n",
    "\n",
    "In this exercise we will study the $k$-means task. Recall that, given an input dataset $\\mathbf{X}$ and a set of centers $\\mathbf{C}$, the $k$-means cost function is defined as $$L(\\mathbf{X}, \\mathbf{C}) = \\sum_{x \\in \\mathbf{X}} \\min_{c \\in \\mathbf{C}} ||x - c||^2.$$\n",
    "\n",
    "We'll start with some theoretical exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f918de7",
   "metadata": {},
   "source": [
    "# Ex1: k-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4dbe18",
   "metadata": {},
   "source": [
    "1. Given a set of points $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ and a set of centers $\\mathbf{C} \\in \\mathbb{R}^{k \\times d}$, we can define an assignment matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times k}$. This matrix is 1 at $i, j$ if $x_i$ is closest to center $c_j$ and 0 otherwise. Suppose we take a step of Lloyd's algorithm and $\\mathbf{A}$ does not change. What can you say about the loss value if we keep doing iterations of Lloyd's algorithm? \n",
    "2. Assume $k > n$. Initialize $\\mathbf{C}$ such that, for every point $x_i$, there exists a center $c_j$ that only has $x_i$ assigned to it. Prove that Lloyd's algorithm on this $\\mathbf{X}$ and $\\mathbf{C}$ will always converge to 0 cost.\n",
    "\n",
    "For the next questions, assume that $k=3$ and we are given $\\mathbf{X} \\in \\mathbb{R}^{5 \\times 2} = \\{(b, 0), (0, b), (0, 0), (-1, 0), (0, -1)\\}$, for $b \\gg 1$ (read: \"$b$ much bigger than $1$\").\n",
    "\n",
    "3. What is the smallest loss that can be obtained?\n",
    "4. What is the largest loss value that Lloyd's algorithm may converge to?\n",
    "5. Can you give an initialization that would lead to this?\n",
    "6. Use the counterexample from above and argue that $k$-means++ should avoid the worst-case result of Lloyd's algorithm with random initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db25ac00",
   "metadata": {},
   "source": [
    "### Looking forward\n",
    "\n",
    "We will investigate variants of questions 3-6 through the rest of this exercise set. To do this, we will design two datasets -- one that is reasonable for Lloyd's algorithm and one that we expect will break Lloyd's algorithm. We will then see whether $k$-means++ can 'fix' Lloyd's algorithm on the 'bad' dataset.\n",
    "\n",
    "We start by defining some helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08056dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize a random $k$-Means solution by sampling randomly from the input dataset.\n",
    "def init_random(x, k):\n",
    "    d = int(x.shape[1])\n",
    "    random_samples = np.random.choice(len(x), k)\n",
    "    return x[random_samples]\n",
    "\n",
    "# Calculate the Euclidean distance between points a and b\n",
    "def euc_dist(a, b):\n",
    "    return np.sqrt(np.sum(np.square(a - b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af818f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting function\n",
    "def plot_lloyds(x, all_centers=None):\n",
    "    plt.scatter(x[:, 0], x[:, 1], alpha=0.8, s=10)\n",
    "    if all_centers:\n",
    "        colors = plt.get_cmap('Reds')(np.linspace(0, 1, len(all_centers)))\n",
    "        for i, centers in enumerate(all_centers):\n",
    "            if i == len(all_centers) - 1:\n",
    "                m = '*'\n",
    "            else:\n",
    "                m = '.'\n",
    "            if i > 0:\n",
    "                for j in range(len(centers)):\n",
    "                    plt.plot(\n",
    "                        [centers[j][0], all_centers[i-1][j][0]],\n",
    "                        [centers[j][1], all_centers[i-1][j][1]],\n",
    "                        linewidth=1,\n",
    "                        marker=m,\n",
    "                        markersize=15 * (i+1)/len(all_centers),\n",
    "                        color=colors[i],\n",
    "                        markeredgecolor=colors[i],\n",
    "                    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499970bf",
   "metadata": {},
   "source": [
    "# Dataset descriptions:\n",
    "We now define the datasets that we will be working with in this exercise. The first is an artificially bad dataset that is designed to 'break' Lloyd's algorithm with naive initialization. $\\frac{1}{3}$ of the clusters in the bad dataset have many points while the remaining $\\frac{2}{3}$ of the clusters have one point each. Consider that we can set these clusters as far apart as we want.\n",
    "\n",
    "The second dataset is a 'well-clusterable' dataset. It is designed to have nice clusters that are easy to find. We expect that Lloyd's algorithm will perform reasonably well on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cc85ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "def create_artificial_bad_dataset(n, k=15, vert_spacing=100, horiz_spacing=None):\n",
    "    \"\"\"\n",
    "    Create a dataset of k clusters where 1/3 of the clusters have many points.\n",
    "    The other clusters each have size 1 point.\n",
    "    All clusters have small variance, so they are 'tightly packed'.\n",
    "    Lloyd's algorithm should perform arbitrarily poorly on this dataset.\n",
    "    \"\"\"\n",
    "    if k % 3 != 0:\n",
    "        raise ValueError(\"Artificial bad dataset needs k to be divisible by 3\")\n",
    "    if horiz_spacing is None:\n",
    "        horiz_spacing = 6 * vert_spacing / k\n",
    "    \n",
    "    # assume dimensionality is 2 so it's easy to plot\n",
    "    points = np.zeros((n, 2))\n",
    "    for column in range(int(k/3)):\n",
    "        x = horiz_spacing * column\n",
    "        points[int(column * 3 * n / k)] = [column * horiz_spacing, vert_spacing]\n",
    "        points[int((column+1) * 3 * n / k - 1)] = [column * horiz_spacing, -vert_spacing]\n",
    "        points[int(column * 3 * n / k + 1) : int((column+1) * 3 * n / k - 1)] = np.random.multivariate_normal([column * horiz_spacing, 0], [[1, 0], [0, 1]], [int(3 * n / k - 2)]) \n",
    "        \n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a076dbf3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the artificially bad dataset\n",
    "artificial_bad_x = create_artificial_bad_dataset(100)\n",
    "plot_lloyds(artificial_bad_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6d5305",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "def create_hierarchical_clusters(n, point=None, std_dev=100, d=2, recur_likelihood=1.0):\n",
    "    \"\"\"\n",
    "    Create a dataset that samples from a Gaussian distribution.\n",
    "    For each sample, recursively sample points from its neighborhood with some likelihood.\n",
    "    In expectation, this creates a normally distributed ball of Gaussian clusters\n",
    "        such that each cluster likely has sub-clusters within it.\n",
    "    \"\"\"\n",
    "    if point is None:\n",
    "        point = np.zeros(d)\n",
    "    if n <= 1:\n",
    "        return np.random.multivariate_normal(point, np.eye(d) * std_dev, 1)\n",
    "    \n",
    "    points = []\n",
    "    points_remaining = n\n",
    "    i = 0\n",
    "    while points_remaining > 1:\n",
    "        subcluster_size = int(np.random.uniform() * points_remaining)\n",
    "        if np.random.uniform() < recur_likelihood:\n",
    "            subcluster_mean = np.random.multivariate_normal(point, np.eye(d) * std_dev)\n",
    "            subcluster = create_hierarchical_clusters(\n",
    "                n=subcluster_size,\n",
    "                point=subcluster_mean,\n",
    "                std_dev=std_dev/10,\n",
    "                d=d,\n",
    "                recur_likelihood=recur_likelihood * np.random.uniform()\n",
    "            )\n",
    "        else:\n",
    "            subcluster = np.random.multivariate_normal(point, np.eye(d) * std_dev, subcluster_size)\n",
    "        points.append(subcluster)\n",
    "        points_remaining -= subcluster_size\n",
    "            \n",
    "    if points:\n",
    "        points = np.concatenate(points, axis=0)\n",
    "        if len(points) > n:\n",
    "            points = points[np.random.choice(len(points), n)]\n",
    "        return points\n",
    "    return np.empty([0, d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a827b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "well_clusterable_x = create_hierarchical_clusters(100, d=2)\n",
    "plot_lloyds(well_clusterable_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b25a18",
   "metadata": {},
   "source": [
    "# Lloyd's Algorithm\n",
    "We will now implement Lloyd's algorithm. This will be done in several sub-methods that we implement first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f7bfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_assignments(points, centers):\n",
    "    '''\n",
    "    Return a vector of length n where the i-th value says which center x_i is assigned to.\n",
    "    For example, if the first point is closest to the fifth center, then assignments[0] = 4.\n",
    "    '''\n",
    "    assignments = -1 * np.ones(len(points))\n",
    "    \n",
    "    # Tip: we defined a function for the euclidean distance in the first cell of this notebook.\n",
    "    ### YOUR CODE HERE\n",
    "    ### END CODE\n",
    "\n",
    "    return assignments.astype(np.int32)\n",
    "\n",
    "def score(points, centers):\n",
    "    '''\n",
    "    Given points and centers, return the total cost of the kmeans objective\n",
    "    '''\n",
    "    assignments = get_assignments(points, centers)\n",
    "    cost = 0\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    ### END CODE\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def lloyds_step(points, centers):\n",
    "    '''\n",
    "    Given the points and centers, get the new set of centers.\n",
    "    This means that we:\n",
    "        1. Find which points are closest to each center using the get_assignments() method above\n",
    "        2. Find the mean of each cluster\n",
    "        3. Update each center to the mean of its cluster\n",
    "    '''\n",
    "    n, d = points.shape\n",
    "    k = centers.shape[0]\n",
    "    assignments = get_assignments(points, centers)\n",
    "    new_centers = np.zeros((k, d))\n",
    "    \n",
    "    # Assign new centers to the means of the clusters\n",
    "    # Tip: consider using np.where(assignments == c) to find the indices of the points that belong to center c\n",
    "    #                                                for 0 <= c < k\n",
    "    ### YOUR CODE HERE\n",
    "    ### END CODE\n",
    "        \n",
    "    return new_centers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7be649",
   "metadata": {},
   "source": [
    "We now put these methods together into Lloyd's algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c740402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lloyds(x, k, init, tol=1e-8, max_steps=1000, loud=True):\n",
    "    all_centers = [init(x, k)]\n",
    "    step = 0\n",
    "    cost = score(x, all_centers[-1])\n",
    "    loss_change = np.inf\n",
    "    while loss_change > tol and step < max_steps:\n",
    "        # Do a step of Lloyd's algorithm and evaluate our new loss\n",
    "        new_centers = lloyds_step(x, all_centers[-1])\n",
    "        new_cost = score(x, new_centers)\n",
    "        loss_change = cost - new_cost\n",
    "        cost = new_cost\n",
    "        \n",
    "        step += 1\n",
    "        all_centers.append(new_centers)\n",
    "        if loud:\n",
    "            print(cost)\n",
    "        \n",
    "    return all_centers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66ba976",
   "metadata": {},
   "source": [
    "### Visualization Description:\n",
    "When plotting Lloyd's algorithm, we represent the final set of centers with stars. We then use a streak of red lines to show how the centers moved. The printed values are the loss values at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee737b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_centers = lloyds(well_clusterable_x, 10, init_random)\n",
    "plot_lloyds(well_clusterable_x, all_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e23076a",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 15\n",
    "all_centers = lloyds(artificial_bad_x, k, init_random)\n",
    "plot_lloyds(artificial_bad_x, all_centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0c1ce7",
   "metadata": {},
   "source": [
    "# $k$-Means++\n",
    "Next, we will implement $k$-means++. Recall that $k$-means++ initializes center $c_i$ based on the distances of the points to centers $c_1$ through $c_{i-1}$. More specifically, given a set of points $\\mathbf{X}$ and a set of $l$ centers $\\mathbf{C}$, we define a probability distribution:\n",
    "$$ p(x_i) = \\frac{\\min_{c \\in \\mathbf{C}} ||x_i - c||^2}{\\sum_{x \\in \\mathbf{X}} \\min_{c \\in \\mathbf{C}} ||x - c||^2}. $$\n",
    "\n",
    "We then sample the next center from the set of points based on this probability distribution. The first center is chosen uniformly at random from $\\mathbf{X}$.\n",
    "\n",
    "## Questions:\n",
    "- Is a point ever going to get sampled twice?\n",
    "- Suppose there are $n-2$ points at the origin, one point at (0, 1) and another at (1, 0). How many steps of Lloyd's algoirthm are required to guarantee 0 loss if we use $k$-means++ initialization with $k=3$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14d6609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min_dists_to_centers(points, new_center, dists):\n",
    "    '''\n",
    "    Return distances of points to centers given a new center.\n",
    "    \n",
    "    If the i-th point has distance 10 to its closest center, then dists[i] = 10.\n",
    "    \n",
    "    Since we are given the distances of the points to the old centers, we are simply interested in which\n",
    "        points are closest to the new center and updating the dists accordingly.\n",
    "    '''\n",
    "    if dists is None:\n",
    "        # dists = None when we had no centers before. Distance to closest center is then inf\n",
    "        dists = np.ones((len(points))) * np.inf\n",
    "    if len(new_center.shape) == 1:\n",
    "        new_center = np.expand_dims(new_center, axis=0)\n",
    "        \n",
    "    ### YOUR CODE HERE\n",
    "    ### END CODE\n",
    "    \n",
    "    return dists\n",
    "\n",
    "def init_kmeans_plusplus(points, k):\n",
    "    '''\n",
    "    Initialize a set of k centers using k-means++\n",
    "    '''\n",
    "    n, d = int(points.shape[0]), int(points.shape[1])\n",
    "    # np.random.choice(n, p) selects from n items according to probability distribution p\n",
    "    centers = [np.random.choice(n)]\n",
    "    sq_dists = None\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    ### END CODE\n",
    "    \n",
    "    return points[centers]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0aebd48",
   "metadata": {},
   "source": [
    "## Comparing $k$-Means++ and Random Initialization\n",
    "\n",
    "We now compare the results of $k$-means++ and random initialization with Lloyd's algorithm.\n",
    "\n",
    "### Well-Clusterable Data\n",
    "We start with plots on the well-clusterable data. The printed values are the loss values during each step of Lloyd's algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1092c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_centers = lloyds(well_clusterable_x, 10, init_random)\n",
    "plot_lloyds(well_clusterable_x, all_centers)\n",
    "\n",
    "all_centers = lloyds(well_clusterable_x, 10, init_kmeans_plusplus)\n",
    "plot_lloyds(well_clusterable_x, all_centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0529f72",
   "metadata": {},
   "source": [
    "### Artificially bad data\n",
    "Now we compare Lloyd's algorithm with $k$-means++ vs. random initialization on the artificially bad dataset. Note the loss values at the end of Lloyd's convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6e7f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_centers = lloyds(artificial_bad_x, 15, init_random)\n",
    "plot_lloyds(artificial_bad_x, all_centers)\n",
    "\n",
    "all_centers = lloyds(artificial_bad_x, 15, init_kmeans_plusplus)\n",
    "plot_lloyds(artificial_bad_x, all_centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf1df58",
   "metadata": {},
   "source": [
    "## Ex2: PCA on MNIST Digits\n",
    "In this exercise we will experiment with PCA on digits.\n",
    "\n",
    "1. Run PCA on the training data train_dat http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html (use n_components = 784) and use ``plot_images`` to plot the 20 directions with largest variance. Use this PCA model for all subsequent computations. The directions can be found with the attribute ``components_``.\n",
    "2. Take the first 20 data points from the training data and project them onto the first $k$ components for $k \\in \\{1, 2,4,8,16, 32, 64\\}$. Then plot them as images. What do you see?\n",
    "    **Hint:** To project them compute the length of the projection of each point onto the first $k$ directions and then compute for each image compute the linear combination of the first $k$ directions given by these directions ($XZZ^\\intercal$ as in lecture).\n",
    "3. Map all the training data train_dat to 2D (the length of the projection on the first two directions) and make a scatter plot where you color with the label and see if there is some structure (use scatter with ``cmap = plt.cm.Paired``, like ``ax.scatter(x,y, c=lab, cmap=plt.cm.Paired)``)\n",
    "4. Map all training data train_dat to $32$ dimensions and train an SGD Classifier. Observe the test accuracy of the classifier. How should you select target dimension in real life?\n",
    "Use the following classifer (svm loss): ``clf = SGDClassifier(loss=\"hinge\", alpha=0.01, max_iter=200, fit_intercept=True)``\n",
    "\n",
    "Discuss the results. Are they what you expect? Why? Why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5261025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## PCA on digits\n",
    "# In this exercise we will experiment with PCA on digits.\n",
    "\n",
    "# 1. Run PCA on the training data train_dat http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html (use n_components = 784) and use ``plot_images`` to plot the 20 directions with largest variance. Use this PCA model for all subsequent computations. The directions can be found with the attribute ``components_``.\n",
    "# 2. Take the first 20 data points from the training data and project them onto the first $k$ components for $k \\in \\{1, 2,4,8,16, 32, 64\\}$. Then plot them as images. What do you see?\n",
    "#     **Hint:** To project them compute the length of the projection of each point onto the first $k$ directions and then compute for each image compute the linear combination of the first $k$ directions given by these directions (XZZ^T as in lecture).\n",
    "# 3. Map all the training data train_dat to 2D (the length of the projection on the first two directions) and make a scatter plot where you color with the label and see if there is some structure (use scatter with ``cmap = plt.cm.Paired``, like ``ax.scatter(x,y, c=lab, cmap=plt.cm.Paired)``)\n",
    "# 4. Map all training data train_dat to $32$ dimensions and train an SGD Classifier. Observe the test accuracy of the classifier. How should you select target dimension in real life?\n",
    "#    Use the following classifer (svm loss): ``clf = SGDClassifier(loss=\"hinge\", alpha=0.01, max_iter=200, fit_intercept=True)``\n",
    "\n",
    "# Discuss the results. Are they what you expect? Why? Why not.\n",
    "\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "# Load full dataset\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "\n",
    "\n",
    "\n",
    "def plot_images(dat, k=20, size=28):\n",
    "    \"\"\" Plot the first k vectors as 28 x 28 images \"\"\"\n",
    "    x2 = dat[0:k,:].reshape(-1, size, size)\n",
    "    x2 = x2.transpose(1, 0, 2)\n",
    "    fig, ax = plt.subplots(figsize=(20,12))\n",
    "    ax.imshow(x2.reshape(size, -1), cmap='bone')\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([])\n",
    "    plt.show()\n",
    "\n",
    "#data, labels =\n",
    "data = mnist_trainset.data.numpy().reshape((60000, 28*28))\n",
    "labels = mnist_trainset.targets.numpy()\n",
    "\n",
    "# reduce size for speed\n",
    "rp = np.random.permutation(len(labels))\n",
    "train_dat = data[rp[:5000],:]\n",
    "test_dat = data[rp[5000:6000], :]\n",
    "train_lab = labels[rp[:5000]]\n",
    "test_lab = labels[rp[5000:6000]]\n",
    "\n",
    "components = None\n",
    "## TASK 1\n",
    "### YOUR CODE HERE\n",
    "### END CODE\n",
    "print('First 20 directions (eigenvectors X^T X)')\n",
    "plot_images(components, 20)\n",
    "\n",
    "## TASK 2\n",
    "# take the first 20 data point and project them onto the first k components and plot for k in [1, 2,4,8,16, 32, 64]\n",
    "img = train_dat[0:20, :]\n",
    "print('Original images:')\n",
    "plot_images(img, 20)\n",
    "for k in [1, 2, 4, 8, 16, 32, 64]:\n",
    "    ### YOUR CODE HERE\n",
    "    ### END CODE\n",
    "    \n",
    "# map the data to 2D and plot the results colored by label\n",
    "proj = None\n",
    "## TASK 3\n",
    "### YOUR CODE HERE\n",
    "### END CODE\n",
    "fig, ax = plt.subplots(figsize=(20, 12))\n",
    "ax.scatter(proj[:,0], proj[:,1], c=train_lab, cmap=plt.cm.Paired)\n",
    "plt.show()\n",
    "\n",
    "## TASK 4\n",
    "clf = SGDClassifier(loss=\"hinge\", alpha=0.01, max_iter=200, fit_intercept=True)\n",
    "### YOUR CODE HERE\n",
    "### END CODE\n",
    "\n",
    "clf_original = SGDClassifier(loss=\"hinge\", alpha=0.01, max_iter=200, fit_intercept=True)\n",
    "clf_original.fit(train_dat, train_lab)\n",
    "\n",
    "proj_test_dat = test_dat @ components[0:32].T\n",
    "acc = (clf.predict(proj_test_dat) == test_lab).mean()\n",
    "print('Test Accuracy of 32 Dim PCA:', 100*acc)\n",
    "print('Test Accuracy original:',100*((clf_original.predict(test_dat) == test_lab).mean()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
